{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Behavior in an Educational Social Network (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A startup has created a platform where students can find and share learning material for their studies. They collect events from users as they browse the platform.\n",
    "\n",
    "They provided us a sample dataset with navigations events splitted on 9 files with the name **part-[0000x].json** (I only supply two of them on the datasets folder).\n",
    "\n",
    "Our mission is to extract information that allows them to analyze what the users are accessing and how they do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we explore the content of the JSON files to determine the best approach for this case. Each file contains one JSON object per line and they are not separated by commas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Json object sample](img/json_file1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we noticed that not all the objects have the same keys, here is one case that has 3 keys relative to a marketing campaign:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Json object sample](img/json_file2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach to Follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset without a defined schema splitted in 9 JSON files.\n",
    "\n",
    "To have a better understanding of what information has value, the first thing will be load the data in a NoSQL Database **for a deeper exploration**.\n",
    "\n",
    "To achieve this, **we will create a Data Pipeline to load the JSON file into a MondoDB database**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technologies to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python\n",
    "* A MongoDB database\n",
    "* MongoDB Compass\n",
    "* Jupyter Notebook\n",
    "* PyMongo (Python module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON to MongoDB Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Base Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### JSON file to List of Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that process a JSON file line by line and append each object to a Python array, returning a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json files containing one dict per line, it is not a list (comma separated) of dicts\n",
    "\n",
    "from bson import json_util\n",
    "\n",
    "def read_json(file):\n",
    "    \"\"\"Convert JSON to Python objects, which PyMongo will then convert to BSON for sending to MongoDB\"\"\"\n",
    "\n",
    "    dict_list = []\n",
    "    \n",
    "    # reading the JSON data using json_util.loads()\n",
    "    for line in open(file, \"r\"):\n",
    "        dict_list.append(json_util.loads(line))\n",
    "\n",
    "    return dict_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### List of Dictionaries to MongoDB Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for make a connection to a MongoDB server and insert JSON objects with the insert_many() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "def _connect_mongo(host, port, username, password, db):\n",
    "    \"\"\" A util for making a connection to mongo \"\"\"\n",
    "\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s?authSource=admin' % (username, password, host, port, db)\n",
    "        conn = MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "\n",
    "\n",
    "    return conn[db]\n",
    "\n",
    "\n",
    "def write_mongo(db, collection, dict_list, host='localhost', port=27017, username=None, password=None):\n",
    "    \"\"\" Read from dict list and Store into MongoDB collection \"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n",
    "    \n",
    "    # !! Delete collection if exists\n",
    "    try:\n",
    "        db[collection].drop()\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # Insert to the specific DB and Collection\n",
    "    \n",
    "    return  db[collection].insert_many(dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Extract function will go through the folder and execute read_json() for each file with the pattern **part-*.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extract all json objects from the files that match the indicated pattern\n",
    "# the function returns an array, thus, we concatenate the returned array to an empty array\n",
    "\n",
    "import glob\n",
    "\n",
    "def extract():\n",
    "\n",
    "    events = []\n",
    "    \n",
    "    for file in glob.glob(\"datasets/part-*.json\"):\n",
    "        \n",
    "        # concatenate arrays (append() will create an array of arrays...)\n",
    "        events = events + read_json(file)\n",
    "    \n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Load function calls write_mongo() with the conection parameters to our MongoDB server and the names of the database and collection that we will create (EducationalPlatform and events). It receives the List of Dictionaries to insert in the database as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(events):\n",
    "\n",
    "    return write_mongo('EducationalPlatform', 'events', events, 'localhost', 27017, 'root', 'root')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pipeline calls the Extract and Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl():\n",
    "    \n",
    "    extracted_data = extract()\n",
    "    \n",
    "    loaded_data = load(extracted_data)\n",
    "    \n",
    "    return loaded_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Main Function Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function will execute the Data Pipeline if we run this notebook from a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For execution we can convert this notebook to a Python script with the following command (from the same folder where the notebook is):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jupyter nbconvert --to script User\\ Behavior\\ in\\ an\\ Educational\\ Social\\ Network\\ \\(Part\\ 1\\).ipynb --output=user_behavior1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we execute the Python script with our Data Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python user_behavior1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have loaded our data into MongoDB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mongodb collection 1](img/mongodb_collection1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mongodb collection 2](img/mongodb_collection2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can analyze the data to decide which fields are relevants to our objective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
